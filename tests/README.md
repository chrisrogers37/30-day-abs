# Testing Suite - 30 Day A/Bs

Welcome to the comprehensive testing suite for 30 Day A/Bs!

## Quick Start

```bash
# Install test dependencies
pip install pytest pytest-cov pytest-asyncio pytest-mock

# Run all tests
pytest

# Run with coverage
pytest --cov=core --cov=llm --cov=schemas --cov-report=html

# Run specific test categories
pytest -m unit              # Unit tests only
pytest -m integration       # Integration tests only
pytest tests/core/          # Core module tests only
```

## Test Structure

```
tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ conftest.py                      # Shared pytest fixtures (20+ fixtures)
â”‚
â”œâ”€â”€ core/                            # Core module tests (25 test files)
â”‚   â”œâ”€â”€ test_analyze.py              # Statistical analysis tests
â”‚   â”œâ”€â”€ test_analyze_business.py     # Business impact analysis tests
â”‚   â”œâ”€â”€ test_analyze_extended.py     # Extended analysis edge cases
â”‚   â”œâ”€â”€ test_analyze_statistical_tests.py  # Chi-square, Fisher's exact tests
â”‚   â”œâ”€â”€ test_design.py               # Sample size calculation tests
â”‚   â”œâ”€â”€ test_design_extended.py      # Extended design edge cases
â”‚   â”œâ”€â”€ test_design_helpers.py       # Design helper function tests
â”‚   â”œâ”€â”€ test_logging.py              # Centralized logging tests
â”‚   â”œâ”€â”€ test_logging_quiz.py         # Quiz session logging tests
â”‚   â”œâ”€â”€ test_question_bank.py        # Question bank tests (50+ questions)
â”‚   â”œâ”€â”€ test_rng.py                  # RNG determinism tests
â”‚   â”œâ”€â”€ test_rng_advanced.py         # Advanced RNG distribution tests
â”‚   â”œâ”€â”€ test_rng_extended.py         # Extended RNG edge cases
â”‚   â”œâ”€â”€ test_scoring.py              # Answer key & scoring tests
â”‚   â”œâ”€â”€ test_scoring_variable.py     # Variable scoring tests
â”‚   â”œâ”€â”€ test_simulate.py             # Data simulation tests
â”‚   â”œâ”€â”€ test_simulate_extended.py    # Extended simulation tests
â”‚   â”œâ”€â”€ test_simulate_utilities.py   # Simulation utility function tests
â”‚   â”œâ”€â”€ test_types.py                # Domain type tests
â”‚   â”œâ”€â”€ test_types_extended.py       # Extended type tests
â”‚   â”œâ”€â”€ test_utils.py                # Utility function tests
â”‚   â”œâ”€â”€ test_utils_extended.py       # Extended utility tests
â”‚   â”œâ”€â”€ test_validation.py           # Answer validation tests
â”‚   â”œâ”€â”€ test_validation_by_id.py     # Validation by question ID tests
â”‚   â””â”€â”€ test_validation_scoring.py   # Validation scoring integration tests
â”‚
â”œâ”€â”€ llm/                             # LLM integration tests (6 test files)
â”‚   â”œâ”€â”€ test_client.py               # LLM client tests
â”‚   â”œâ”€â”€ test_generator.py            # Scenario generation tests
â”‚   â”œâ”€â”€ test_guardrails.py           # Parameter validation tests
â”‚   â”œâ”€â”€ test_integration.py          # LLM pipeline tests
â”‚   â”œâ”€â”€ test_novelty_scoring.py      # Novelty scoring tests
â”‚   â””â”€â”€ test_parser.py               # JSON parsing tests
â”‚
â”œâ”€â”€ schemas/                         # Schema validation tests (7 test files)
â”‚   â”œâ”€â”€ test_analyze.py              # Analysis schema tests
â”‚   â”œâ”€â”€ test_complications.py        # Complications schema tests
â”‚   â”œâ”€â”€ test_design.py               # Design schema tests
â”‚   â”œâ”€â”€ test_evaluation.py           # Evaluation schema tests
â”‚   â”œâ”€â”€ test_scenario.py             # Scenario schema tests
â”‚   â”œâ”€â”€ test_shared.py               # Shared schema tests
â”‚   â””â”€â”€ test_simulate.py             # Simulation schema tests
â”‚
â”œâ”€â”€ ui/                              # UI component tests
â”‚   â””â”€â”€ test_streamlit_app_enhanced.py  # Enhanced Streamlit tests
â”‚
â”œâ”€â”€ integration/                     # E2E integration tests
â”‚   â”œâ”€â”€ test_complete_workflow.py    # Full quiz flow tests
â”‚   â”œâ”€â”€ test_data_export.py          # Data export tests
â”‚   â”œâ”€â”€ test_llm_pipeline.py         # LLM â†’ Core pipeline tests
â”‚   â””â”€â”€ test_real_api.py             # Real API tests (skipped in CI)
â”‚
â”œâ”€â”€ fixtures/                        # Shared test data
â”‚   â”œâ”€â”€ scenarios/                   # Sample scenario JSON files
â”‚   â”‚   â”œâ”€â”€ ecommerce_scenario.json
â”‚   â”‚   â”œâ”€â”€ saas_scenario.json
â”‚   â”‚   â””â”€â”€ fintech_scenario.json
â”‚   â”œâ”€â”€ expected_results.py          # Expected calculation results
â”‚   â”œâ”€â”€ llm_responses.py             # Mock LLM responses
â”‚   â””â”€â”€ test_data.py                 # Reusable test data
â”‚
â”œâ”€â”€ helpers/                         # Test utilities
â”‚   â”œâ”€â”€ assertions.py                # Custom assertions
â”‚   â”œâ”€â”€ factories.py                 # Object factories
â”‚   â””â”€â”€ mocks.py                     # Mock objects
â”‚
â”œâ”€â”€ test_basic.py                    # Quick smoke tests
â”œâ”€â”€ test_notebooks.py                # Notebook template tests
â””â”€â”€ test_streamlit_app.py            # Basic Streamlit import/functionality tests
```

## Test Categories

### Unit Tests (`@pytest.mark.unit`)
Fast, isolated tests for individual functions:
- Core mathematical calculations
- Type validation
- Utility functions
- Schema validation

### Integration Tests (`@pytest.mark.integration`)
Module interaction tests:
- Complete workflow (design â†’ simulate â†’ analyze)
- LLM â†’ Core pipeline
- Data export functionality

### E2E Tests (`@pytest.mark.e2e`)
End-to-end user workflow tests:
- Full quiz flow
- Complete analysis pipeline
- User interaction scenarios

## Coverage Targets

- **Core Module**: 90%+ (critical mathematical functions)
- **LLM Module**: 80%+ (integration logic)
- **Schemas Module**: 85%+ (validation logic)
- **UI Module**: 60%+ (component tests)

## Running Tests

### Basic Commands

```bash
# Run all tests
pytest

# Run with verbose output
pytest -v

# Run with coverage
pytest --cov=core --cov=llm --cov=schemas

# Generate HTML coverage report
pytest --cov=core --cov=llm --cov=schemas --cov-report=html
open htmlcov/index.html
```

### Test Selection

```bash
# By marker
pytest -m unit                    # Unit tests only
pytest -m integration             # Integration tests only
pytest -m "not slow"              # Exclude slow tests
pytest -m "not requires_api"      # Exclude tests needing API keys

# By module
pytest tests/core/                # Core module tests
pytest tests/llm/                 # LLM module tests
pytest tests/integration/         # Integration tests

# By file
pytest tests/core/test_design.py  # Specific file

# By function
pytest tests/core/test_design.py::TestComputeSampleSize::test_compute_sample_size_basic
```

### Output Options

```bash
# Show print statements
pytest -s

# Stop on first failure
pytest -x

# Show local variables on failure
pytest -l

# Quiet output
pytest -q

# Show test durations
pytest --durations=10
```

## Test Fixtures

Shared fixtures are defined in `conftest.py`:

- **Design Parameters**: `standard_design_params`, `high_baseline_design_params`, `low_baseline_design_params`
- **Allocations**: `standard_allocation`, `unbalanced_allocation`
- **Simulation Results**: `simple_sim_result`, `significant_sim_result`, `non_significant_sim_result`
- **Tolerances**: `tolerance_percentage`, `tolerance_absolute`
- **Mock Data**: `sample_scenario_dict`, `mock_llm_response_json`

## Writing Tests

### Example Unit Test

```python
import pytest
from core.design import compute_sample_size
from tests.helpers.factories import create_design_params

class TestSampleSize:
    @pytest.mark.unit
    def test_compute_sample_size(self):
        params = create_design_params()
        result = compute_sample_size(params)
        
        assert result.per_arm > 0
        assert result.total == 2 * result.per_arm
```

### Example Integration Test

```python
import pytest
from tests.helpers.factories import create_design_params
from core.design import compute_sample_size
from core.simulate import simulate_trial
from core.analyze import analyze_results

class TestWorkflow:
    @pytest.mark.integration
    def test_complete_flow(self):
        # Design
        params = create_design_params()
        sample_size = compute_sample_size(params)
        
        # Simulate
        sim_result = simulate_trial(params, seed=42)
        
        # Analyze
        analysis = analyze_results(sim_result, alpha=0.05)
        
        assert analysis.p_value >= 0.0
        assert analysis.p_value <= 1.0
```

## Documentation

For detailed testing documentation, see:

- **[Testing Guide](../development_docs/TESTING_GUIDE.md)**: Comprehensive testing documentation
- **[Development Guide](../development_docs/DEVELOPMENT_GUIDE.md)**: Development workflow and standards
- **[Development Docs README](../development_docs/README.md)**: Navigation guide

## Contributing

When contributing tests:

1. Follow the existing test structure
2. Use appropriate markers (`@pytest.mark.unit`, etc.)
3. Use fixtures from `conftest.py`
4. Use helpers from `tests/helpers/`
5. Maintain test coverage above thresholds
6. Ensure all tests pass before submitting PR

## Troubleshooting

### Common Issues

**Issue**: Tests not found
**Solution**: Ensure you're running pytest from project root

**Issue**: Import errors
**Solution**: Add project root to PYTHONPATH:
```bash
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
```

**Issue**: Fixture not found
**Solution**: Check `conftest.py` for available fixtures

**Issue**: Coverage not working
**Solution**: Install pytest-cov:
```bash
pip install pytest-cov
```

## CI/CD

Tests are configured to run in CI/CD pipelines:

- **Unit tests**: Run on every commit
- **Integration tests**: Run on pull requests
- **Real API tests**: Run manually or nightly (requires API keys)

See `.github/workflows/test.yml` for CI configuration (when set up).

---

**Happy Testing!** ðŸ§ª

For questions or issues, see the [Development Guide](../development_docs/DEVELOPMENT_GUIDE.md) or create an issue on GitHub.

